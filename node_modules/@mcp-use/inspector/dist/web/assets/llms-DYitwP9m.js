import{x as O,y as M,z as R,n as b,C as L,G as N,B as x,D as J,R as P}from"./index-B9xgKKTO.js";import{O as k,d as E,e as T}from"./index-D8-Z6Ssp.js";function C(n,e){if(n.function===void 0)return;let t;if(e?.partial)try{t=R(n.function.arguments??"{}")}catch{return}else try{t=JSON.parse(n.function.arguments)}catch(r){throw new k([`Function "${n.function.name}" arguments:`,"",n.function.arguments,"","are not valid JSON.",`Error: ${r.message}`].join(`
`))}const s={name:n.function.name,args:t,type:"tool_call"};return e?.returnId&&(s.id=n.id),s}function z(n){if(n.id===void 0)throw new Error('All OpenAI tool calls must have an "id" field.');return{id:n.id,type:"function",function:{name:n.name,arguments:JSON.stringify(n.args)}}}function A(n,e){return{name:n.function?.name,args:n.function?.arguments,id:n.id,error:e,type:"invalid_tool_call"}}var S=class extends E{static lc_name(){return"JsonOutputToolsParser"}returnId=!1;lc_namespace=["langchain","output_parsers","openai_tools"];lc_serializable=!0;constructor(n){super(n),this.returnId=n?.returnId??this.returnId}_diff(){throw new Error("Not supported.")}async parse(){throw new Error("Not implemented.")}async parseResult(n){return await this.parsePartialResult(n,!1)}async parsePartialResult(n,e=!0){const t=n[0].message;let s;if(M(t)&&t.tool_calls?.length?s=t.tool_calls.map(a=>{const{id:l,...u}=a;return this.returnId?{id:l,...u}:u}):t.additional_kwargs.tool_calls!==void 0&&(s=JSON.parse(JSON.stringify(t.additional_kwargs.tool_calls)).map(l=>C(l,{returnId:this.returnId,partial:e}))),!s)return[];const r=[];for(const a of s)if(a!==void 0){const l={type:a.name,args:a.args,id:a.id};r.push(l)}return r}},B=class extends S{static lc_name(){return"JsonOutputKeyToolsParser"}lc_namespace=["langchain","output_parsers","openai_tools"];lc_serializable=!0;returnId=!1;keyName;returnSingle=!1;zodSchema;constructor(n){super(n),this.keyName=n.keyName,this.returnSingle=n.returnSingle??this.returnSingle,this.zodSchema=n.zodSchema}async _validateResult(n){if(this.zodSchema===void 0)return n;const e=await O(this.zodSchema,n);if(e.success)return e.data;throw new k(`Failed to parse. Text: "${JSON.stringify(n,null,2)}". Error: ${JSON.stringify(e.error?.issues)}`,JSON.stringify(n,null,2))}async parsePartialResult(n){const t=(await super.parsePartialResult(n)).filter(r=>r.type===this.keyName);let s=t;if(t.length)return this.returnId||(s=t.map(r=>r.args)),this.returnSingle?s[0]:s}async parseResult(n){const t=(await super.parsePartialResult(n,!1)).filter(a=>a.type===this.keyName);let s=t;return t.length?(this.returnId||(s=t.map(a=>a.args)),this.returnSingle?this._validateResult(s[0]):await Promise.all(s.map(a=>this._validateResult(a)))):void 0}},F={};b(F,{JsonOutputKeyToolsParser:()=>B,JsonOutputToolsParser:()=>S,convertLangChainToolCallToOpenAI:()=>z,makeInvalidToolCall:()=>A,parseToolCall:()=>C});var K={};b(K,{BaseLLM:()=>I,LLM:()=>U});var I=class f extends T{lc_namespace=["langchain","llms",this._llmType()];async invoke(e,t){const s=f._convertInputToPromptValue(e);return(await this.generatePrompt([s],t,t?.callbacks)).generations[0][0].text}async*_streamResponseChunks(e,t,s){throw new Error("Not implemented.")}_separateRunnableConfigFromCallOptionsCompat(e){const[t,s]=super._separateRunnableConfigFromCallOptions(e);return s.signal=t.signal,[t,s]}async*_streamIterator(e,t){if(this._streamResponseChunks===f.prototype._streamResponseChunks)yield this.invoke(e,t);else{const s=f._convertInputToPromptValue(e),[r,a]=this._separateRunnableConfigFromCallOptionsCompat(t),l=await L.configure(r.callbacks,this.callbacks,r.tags,this.tags,r.metadata,this.metadata,{verbose:this.verbose}),u={options:a,invocation_params:this?.invocationParams(a),batch_size:1},h=await l?.handleLLMStart(this.toJSON(),[s.toString()],r.runId,void 0,u,void 0,void 0,r.runName);let o=new N({text:""});try{for await(const i of this._streamResponseChunks(s.toString(),a,h?.[0]))o?o=o.concat(i):o=i,typeof i.text=="string"&&(yield i.text)}catch(i){throw await Promise.all((h??[]).map(d=>d?.handleLLMError(i))),i}await Promise.all((h??[]).map(i=>i?.handleLLMEnd({generations:[[o]]})))}}async generatePrompt(e,t,s){const r=e.map(a=>a.toString());return this.generate(r,t,s)}invocationParams(e){return{}}_flattenLLMResult(e){const t=[];for(let s=0;s<e.generations.length;s+=1){const r=e.generations[s];if(s===0)t.push({generations:[r],llmOutput:e.llmOutput});else{const a=e.llmOutput?{...e.llmOutput,tokenUsage:{}}:void 0;t.push({generations:[r],llmOutput:a})}}return t}async _generateUncached(e,t,s,r){let a;if(r!==void 0&&r.length===e.length)a=r;else{const o=await L.configure(s.callbacks,this.callbacks,s.tags,this.tags,s.metadata,this.metadata,{verbose:this.verbose}),i={options:t,invocation_params:this?.invocationParams(t),batch_size:e.length};a=await o?.handleLLMStart(this.toJSON(),e,s.runId,void 0,i,void 0,void 0,s?.runName)}const l=!!a?.[0].handlers.find(x);let u;if(l&&e.length===1&&this._streamResponseChunks!==f.prototype._streamResponseChunks)try{const o=await this._streamResponseChunks(e[0],t,a?.[0]);let i;for await(const d of o)i===void 0?i=d:i=J(i,d);if(i===void 0)throw new Error("Received empty response from chat model call.");u={generations:[[i]],llmOutput:{}},await a?.[0].handleLLMEnd(u)}catch(o){throw await a?.[0].handleLLMError(o),o}else{try{u=await this._generate(e,t,a?.[0])}catch(i){throw await Promise.all((a??[]).map(d=>d?.handleLLMError(i))),i}const o=this._flattenLLMResult(u);await Promise.all((a??[]).map((i,d)=>i?.handleLLMEnd(o[d])))}const h=a?.map(o=>o.runId)||void 0;return Object.defineProperty(u,P,{value:h?{runIds:h}:void 0,configurable:!0}),u}async _generateCached({prompts:e,cache:t,llmStringKey:s,parsedOptions:r,handledOptions:a,runId:l}){const u=await L.configure(a.callbacks,this.callbacks,a.tags,this.tags,a.metadata,this.metadata,{verbose:this.verbose}),h={options:r,invocation_params:this?.invocationParams(r),batch_size:e.length},o=await u?.handleLLMStart(this.toJSON(),e,l,void 0,h,void 0,void 0,a?.runName),i=[],v=(await Promise.allSettled(e.map(async(c,m)=>{const _=await t.lookup(c,s);return _==null&&i.push(m),_}))).map((c,m)=>({result:c,runManager:o?.[m]})).filter(({result:c})=>c.status==="fulfilled"&&c.value!=null||c.status==="rejected"),p=[];await Promise.all(v.map(async({result:c,runManager:m},_)=>{if(c.status==="fulfilled"){const y=c.value;return p[_]=y.map(w=>(w.generationInfo={...w.generationInfo,tokenUsage:{}},w)),y.length&&await m?.handleLLMNewToken(y[0].text),m?.handleLLMEnd({generations:[y]},void 0,void 0,void 0,{cached:!0})}else return await m?.handleLLMError(c.reason,void 0,void 0,void 0,{cached:!0}),Promise.reject(c.reason)}));const g={generations:p,missingPromptIndices:i,startedRunManagers:o};return Object.defineProperty(g,P,{value:o?{runIds:o?.map(c=>c.runId)}:void 0,configurable:!0}),g}async generate(e,t,s){if(!Array.isArray(e))throw new Error("Argument 'prompts' is expected to be a string[]");let r;Array.isArray(t)?r={stop:t}:r=t;const[a,l]=this._separateRunnableConfigFromCallOptionsCompat(r);if(a.callbacks=a.callbacks??s,!this.cache)return this._generateUncached(e,l,a);const{cache:u}=this,h=this._getSerializedCacheKeyParametersForCall(l),{generations:o,missingPromptIndices:i,startedRunManagers:d}=await this._generateCached({prompts:e,cache:u,llmStringKey:h,parsedOptions:l,handledOptions:a,runId:a.runId});let v={};if(i.length>0){const p=await this._generateUncached(i.map(g=>e[g]),l,a,d!==void 0?i.map(g=>d?.[g]):void 0);await Promise.all(p.generations.map(async(g,c)=>{const m=i[c];return o[m]=g,u.update(e[m],h,g)})),v=p.llmOutput??{}}return{generations:o,llmOutput:v}}_identifyingParams(){return{}}_modelType(){return"base_llm"}},U=class extends I{async _generate(n,e,t){return{generations:await Promise.all(n.map((r,a)=>this._call(r,{...e,promptIndex:a},t).then(l=>[{text:l}])))}}};export{I as B,B as J,U as L,z as c,K as l,A as m,F as o,C as p};
